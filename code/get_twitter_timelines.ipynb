{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7df11ea-dde5-4bde-992c-5f0b4bb42cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06354d31-f480-447f-95a2-d653305a5770",
   "metadata": {},
   "source": [
    "# Get user timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85993ef1-b1ee-4fae-a339-112513b95bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../data\"\n",
    "dst = \"../data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "694c6433-18e7-4224-b311-cee81a254197",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"candidate_twitter_profiles.csv\"\n",
    "users = pd.read_csv(\n",
    "    Path(src, \"tmp\", fname),\n",
    "    dtype={\"author_id\":str},\n",
    "    parse_dates=[\"created_at\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "346446ce-2869-417a-a2b6-4d26d2b6bcec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the list of accounts into batches\n",
    "if not os.path.exists(Path(dst, \"tmp\", \"user_batches\")): os.mkdir(Path(dst, \"tmp\", \"user_batches\"))\n",
    "\n",
    "N_keys = 7  \n",
    "batch_size = int(len(users) / N_keys)\n",
    "for i in range(N_keys):\n",
    "    batch = users[\"author_id\"][i * batch_size : (i+ 1) * batch_size]\n",
    "    np.savetxt(Path(dst, \"tmp\", \"user_batches\", f\"candidate_twitter_accounts_batch_{i}.txt\"), \n",
    "               batch, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec6d54-374d-4924-a2d4-80918fc3ea2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the account ID batches to the remote server\n",
    "! rsync -avze ssh ../data/tmp/user_batches/candidate_twitter_accounts_batch_* jlasser@medea:/data/honesty/corpora/Twitter/midterm_candidate_accounts/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb037d28-c8ae-473c-8ed7-9f8b52c6bf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commands on remote server to download the user timelines for each batch using \n",
    "# twarc2 - only works with a valid bearer token\n",
    "cd /data/honesty/corpora/Twitter/midterm_candidate_accounts\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time 2022-01-01 --end-time 2023-05-01 --use-search candidate_twitter_accounts_batch_0.txt ../midterm_candidate_timelines/midterm_candidate_timelines_batch_0.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time 2022-01-01 --end-time 2023-05-01 --use-search candidate_twitter_accounts_batch_1.txt ../midterm_candidate_timelines/midterm_candidate_timelines_batch_1.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time 2022-01-01 --end-time 2023-05-01 --use-search candidate_twitter_accounts_batch_2.txt ../midterm_candidate_timelines/midterm_candidate_timelines_batch_2.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time 2022-01-01 --end-time 2023-05-01 --use-search candidate_twitter_accounts_batch_3.txt ../midterm_candidate_timelines/midterm_candidate_timelines_batch_3.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time 2022-01-01 --end-time 2023-05-01 --use-search candidate_twitter_accounts_batch_4.txt ../midterm_candidate_timelines/midterm_candidate_timelines_batch_4.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time 2022-01-01 --end-time 2023-05-01 --use-search candidate_twitter_accounts_batch_5.txt ../midterm_candidate_timelines/midterm_candidate_timelines_batch_5.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time 2022-01-01 --end-time 2023-05-01 --use-search candidate_twitter_accounts_batch_6.txt ../midterm_candidate_timelines/midterm_candidate_timelines_batch_6.jsonl\n",
    "twarc2 --bearer-token XXX timelines --no-context-annotations --start-time 2022-01-01 --end-time 2023-05-01 --use-search candidate_twitter_accounts_batch_7.txt ../midterm_candidate_timelines/midterm_candidate_timelines_batch_7.jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6558b1-89c9-4c27-bc6d-12fc4e3fb18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commands on the server to convert the raw .json files to .csv\n",
    "cd /data/honesty/corpora/Twitter/midterm_candidate_timelines\n",
    "twarc2 csv --extra-input-columns \"edit_history_tweet_ids,public_metrics.impression_count\" --input-data-type tweets midterm_candidate_timelines_batch_0.jsonl midterm_candidate_timelines_batch_0.csv\n",
    "twarc2 csv --extra-input-columns \"edit_history_tweet_ids,public_metrics.impression_count\" --input-data-type tweets midterm_candidate_timelines_batch_1.jsonl midterm_candidate_timelines_batch_1.csv\n",
    "twarc2 csv --extra-input-columns \"edit_history_tweet_ids,public_metrics.impression_count\" --input-data-type tweets midterm_candidate_timelines_batch_2.jsonl midterm_candidate_timelines_batch_2.csv\n",
    "twarc2 csv --extra-input-columns \"edit_history_tweet_ids,public_metrics.impression_count\" --input-data-type tweets midterm_candidate_timelines_batch_3.jsonl midterm_candidate_timelines_batch_3.csv\n",
    "twarc2 csv --extra-input-columns \"edit_history_tweet_ids,public_metrics.impression_count\" --input-data-type tweets midterm_candidate_timelines_batch_4.jsonl midterm_candidate_timelines_batch_4.csv\n",
    "twarc2 csv --extra-input-columns \"edit_history_tweet_ids,public_metrics.impression_count\" --input-data-type tweets midterm_candidate_timelines_batch_5.jsonl midterm_candidate_timelines_batch_5.csv\n",
    "twarc2 csv --extra-input-columns \"edit_history_tweet_ids,public_metrics.impression_count\" --input-data-type tweets midterm_candidate_timelines_batch_6.jsonl midterm_candidate_timelines_batch_6.csv\n",
    "twarc2 csv --extra-input-columns \"edit_history_tweet_ids,public_metrics.impression_count\" --input-data-type tweets midterm_candidate_timelines_batch_7.jsonl midterm_candidate_timelines_batch_7.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "679049cf-f8f8-4451-a7a4-95f02e3ea64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# commands on the server to compress the .csv files\n",
    "xz -v midterm_candidate_timelines_batch_0.csv\n",
    "xz -v midterm_candidate_timelines_batch_0.jsonl\n",
    "xz -v midterm_candidate_timelines_batch_1.csv\n",
    "xz -v midterm_candidate_timelines_batch_1.jsonl\n",
    "xz -v midterm_candidate_timelines_batch_2.csv\n",
    "xz -v midterm_candidate_timelines_batch_2.jsonl\n",
    "xz -v midterm_candidate_timelines_batch_3.csv\n",
    "xz -v midterm_candidate_timelines_batch_3.jsonl\n",
    "xz -v midterm_candidate_timelines_batch_4.csv\n",
    "xz -v midterm_candidate_timelines_batch_4.jsonl\n",
    "xz -v midterm_candidate_timelines_batch_5.csv\n",
    "xz -v midterm_candidate_timelines_batch_5.jsonl\n",
    "xz -v midterm_candidate_timelines_batch_6.csv\n",
    "xz -v midterm_candidate_timelines_batch_6.jsonl\n",
    "xz -v midterm_candidate_timelines_batch_7.csv\n",
    "xz -v midterm_candidate_timelines_batch_7.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2103399e-4f47-42d5-ba3b-83a170749505",
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir -p ../data/tmp/midterm_candidate_timelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710abd6f-48c2-47a8-971f-9c8c25b5bf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the .csv files from the server\n",
    "! rsync -avze ssh jlasser@medea:/data/honesty/corpora/Twitter/midterm_candidate_timelines/*csv.xz ../data/tmp/midterm_candidate_timelines --progress "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efb19a9-05e1-49c8-8bf4-49b6c490f7b5",
   "metadata": {},
   "source": [
    "# Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a4e9a3b-3c7c-49cd-b9d6-a6020ebb4a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = listdir(Path(src, \"tmp\", \"midterm_candidate_timelines\"))\n",
    "fnames = [f for f in fnames if f.endswith(\"csv.xz\")]\n",
    "timelines = pd.concat([pd.read_csv(\n",
    "    Path(src, \"tmp\", \"midterm_candidate_timelines\", fname), \n",
    "    compression=\"xz\", \n",
    "    dtype={\"id\":str, \"author_id\":str}, \n",
    "    low_memory=False\n",
    ") for fname in fnames])\n",
    "timelines = timelines.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af4a36ed-b1c2-4cde-98f1-374f0cf11ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 1060 tweets from unknown users\n"
     ]
    }
   ],
   "source": [
    "N = len(timelines)\n",
    "timelines = timelines[timelines[\"author_id\"].isin(users[\"author_id\"])]\n",
    "print(f\"dropped {N - len(timelines)} tweets from unknown users\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7c98c3b-fdd5-4ff1-90d8-1821672453cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "timelines[\"created_at\"] = pd.to_datetime(timelines[\"created_at\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "942a5480-49bc-4414-8a42-0c0d0b11bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tweet type\n",
    "timelines[\"retweeted\"] = False\n",
    "timelines[\"quoted\"] = False\n",
    "timelines[\"reply\"] = False\n",
    "timelines.loc[timelines[\"referenced_tweets.retweeted.id\"].dropna().index, \"retweeted\"] = True\n",
    "timelines.loc[timelines[\"referenced_tweets.quoted.id\"].dropna().index, \"quoted\"] = True\n",
    "timelines.loc[timelines[\"referenced_tweets.replied_to.id\"].dropna().index, \"reply\"] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd1eb00b-ad8a-45f7-9bbd-6834b2508894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 0 tweets outside the desired time window\n"
     ]
    }
   ],
   "source": [
    "# make sure all tweets are within the desired time window\n",
    "start = pd.to_datetime(\"2022-01-01\", utc=\"UTC\")\n",
    "end = pd.to_datetime(\"2023-05-01\", utc=\"UTC\")\n",
    "N = len(timelines)\n",
    "timelines = timelines[timelines[\"created_at\"] >= start]\n",
    "timelines = timelines[timelines[\"created_at\"] < end]\n",
    "print(f\"dropped {N - len(timelines)} tweets outside the desired time window\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b40cf89-c250-4e41-8bd6-2e416213d2f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 166389 duplicates\n"
     ]
    }
   ],
   "source": [
    "# drop duplicates\n",
    "N = len(timelines)\n",
    "timelines = timelines.sort_values(\"__twarc.retrieved_at\", ascending=False)\n",
    "timelines = timelines.drop_duplicates(subset=[\"id\"])\n",
    "print(f\"dropped {N - len(timelines)} duplicates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28e41422-1346-4c29-82e8-5e5320de353b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 436280 retweets\n"
     ]
    }
   ],
   "source": [
    "# drop retweets\n",
    "N = len(timelines)\n",
    "timelines = timelines[timelines[\"retweeted\"] == False]\n",
    "print(f\"dropped {N - len(timelines)} retweets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3d695b5-a59a-4b3a-b8dc-ddc3f27bf026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1084776 tweets remaining\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(timelines)} tweets remaining\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2812b9ff-cb5d-4739-a50a-6419e549fe0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up column names\n",
    "timelines = timelines.rename(columns={\n",
    "    'public_metrics.like_count':'like_count',\n",
    "    'public_metrics.reply_count':'reply_count',\n",
    "    'public_metrics.retweet_count':'retweet_count',\n",
    "    'public_metrics.quote_count':'quote_count',\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ea2fa255-98a7-4c27-b160-be31a1d89fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract URL objects\n",
    "urls = []\n",
    "expanded_urls = []\n",
    "for obj in timelines[\"entities.urls\"]:\n",
    "    if obj != obj:\n",
    "        urls.append([])\n",
    "        expanded_urls.append([])\n",
    "    else:\n",
    "        obj = eval(obj)\n",
    "        tmp_urls = []\n",
    "        tmp_expanded_urls = []\n",
    "        for entry in obj:\n",
    "            if type(entry) == dict:\n",
    "                tmp_urls.append(entry[\"url\"])\n",
    "            elif type(entry) == str:\n",
    "                tmp_urls.append(entry)\n",
    "            else:\n",
    "                print(\"unknown entry type!\")\n",
    "                \n",
    "            if type(entry) == dict:\n",
    "                tmp_expanded_urls.append(entry[\"expanded_url\"])\n",
    "            elif type(entry) == str:\n",
    "                tmp_expanded_urls.append(entry)\n",
    "            else:\n",
    "                print(\"unknown entry type!\")\n",
    "                \n",
    "        urls.append(tmp_urls)\n",
    "        expanded_urls.append(tmp_expanded_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9526294f-e782-404e-aa2f-248e36828e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "timelines[\"urls\"] = urls\n",
    "timelines[\"expanded_urls\"] = expanded_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "782751e4-a40d-40b1-9176-9b7fc992bed1",
   "metadata": {},
   "source": [
    "# Export the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7b1fcd15-2da9-48ac-bd64-6fc557d0ad94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"combined_midterm_candidate_timelines_2022-01-01_to_2023-05-01_clean.csv.gzip\"\n",
    "cols = [\"id\", \"author_id\", \"created_at\", \"expanded_urls\", \"retweeted\", \"quoted\",\n",
    "        \"reply\", \"text\", \"retweet_count\", \"reply_count\", \"like_count\",\n",
    "        \"quote_count\"]\n",
    "timelines[cols].to_csv(Path(dst, \"raw\", fname), index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92b0d596-479a-44d3-8df5-a63da4a43393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export tweet IDs for hydration\n",
    "fname = \"combined_midterm_candidate_timelines_2022-01-01_to_2023-05-01_clean_tweetIDs.txt\"\n",
    "np.savetxt(Path(dst, \"raw\", fname), timelines[\"id\"].values, fmt=\"%s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5cb6cc-32f3-4e07-aa65-f18492c2f9fb",
   "metadata": {},
   "source": [
    "# Exploratory analysis: resolve URLs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1bea5c-b9c8-429a-bbe5-1a089c3437bf",
   "metadata": {},
   "source": [
    "## Export URLs to unravel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cb721c3-6d65-4584-b744-9ef79945454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain(url):\n",
    "    if url != url:\n",
    "        return np.nan\n",
    "    # trailing \"/\" and spaces\n",
    "    url = url.strip('/').strip()\n",
    "    # transform all domains to lowercase\n",
    "    url = url.lower()\n",
    "    # remove any white spaces\n",
    "    url = url.replace(' ', '')\n",
    "    # if present: remove the protocol\n",
    "    if url.startswith((\"http\", \"https\")):\n",
    "        try:\n",
    "            url = url.split('//')[1]\n",
    "        except IndexError:\n",
    "            print(f\"found malformed URL {url}\")\n",
    "            return np.nan\n",
    "    # remove \"www.\" \n",
    "    url = url.replace('www.', '')\n",
    "    url = url.split(\"/\")[0]\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16b68ca1-12df-43ba-98e1-505376d525be",
   "metadata": {},
   "outputs": [],
   "source": [
    "URLs = []\n",
    "for url_list in timelines[\"expanded_urls\"]:\n",
    "    URLs.extend(url_list)\n",
    "URLs = pd.DataFrame({\"url\":list(set(URLs))})\n",
    "URLs[\"domain\"] = URLs[\"url\"].apply(extract_domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdafe9da-a098-4396-98e5-9ef4e94b7b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial list of shorteners from this repo:\n",
    "# https://github.com/boutetnico/url-shorteners\n",
    "url_shorteners = list(np.loadtxt(Path(\"utilities\", \"url_shorteners.txt\"), dtype=str))\n",
    "\n",
    "# add URL shorteners based on manual inspections of all URLs that appeared >100\n",
    "# times in the dataset\n",
    "url_shorteners.extend([\n",
    "    \"fb.me\", \"buff.ly\", \"nyti.ms\", \"wapo.st\", \"youtu.be\", \"1.usa.gov\", \"fxn.ws\",\n",
    "    \"on.fb.me\", \"politi.co\", \"trib.al\", \"washex.am\", \"hill.cm\", \"cnb.cx\",\n",
    "    \"hubs.ly\", \"cs.pn\",\"n.pr\", \"conta.cc\", \"mi.tt\", \"usat.ly\", \"abcn.ws\",\n",
    "    \"reut.rs\", \"cbsn.ws\", \"huff.to\", \"instagr.am\", \"bloom.bg\", \"fw.to\", \n",
    "    \"ift.tt\", \"strib.mn\", \"lat.ms\", \"afs.mn\", \"dpo.st\", \"mailchi.mp\",\n",
    "    \"dailysign.al\", \"tmblr.co\", \"rub.io\", \"yhoo.it\", \"omny.fm\", \"chrl.ie\",\n",
    "    \"tulsi.to\", \"apne.ws\", \"hrc.io\", \"ed.gr\", \"ti.me\", \"herit.ag\", \"indy.st\",\n",
    "    \"ofa.bo\", \"trib.in\", \"azc.cc\", \"bsun.md\", \"wjcf.co\", \"bityl.co\", \"go.shr.lc\",\n",
    "    \"t1p.de\", \"m.bild.de\", \"sz.de\", \"m.faz.net\", \"zpr.io\", \"m.tagesspiegel.de\",\n",
    "    \"to.welt.de\", \"gleft.de\", \"nol.is\", \"m.spiegel.de\", \"m.youtube.com\", \n",
    "    \"m.facebook.com\", \"m.focus.de\", \"loom.ly\", \"t.me\", \"4sq.com\", \"diplo.de\",\n",
    "    \"p.dw.com\", \"owl.li\", \"tmi.me\", \"m.haz.de\", \"ly.zdf.de\", \"chng.it\", \"img.ly\",\n",
    "    \"m.augsburger-allgemeine.de\", \"x.swr.de\", \"m.fr.de\", \"ebx.sh\", \"m.fr.de\",\n",
    "    \"fcld.ly\", \"spoti.fi\", \"shar.es\", \"s.rlp.de\", \"m.welt.de\", \"bbc.in\", \n",
    "    \"on.ft.com\", \"fb.watch\", \"mol.im\", \"crowd.in \", \"zcu.io\", \"gu.com\",\n",
    "    \"lnkd.in\", \"shorturl.at\", \"m.huffingtonpost.co.uk\", \"fal.cn\", \"lght.ly\", \n",
    "    \"econ.st\", \"huffp.st\", \"l-bc.co\", \"wbs.wales\", \"aca.st \", \"ind.pn\", \"cutt.ly\",\n",
    "    \"dailym.ai\", \"ow.ly\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c491b593-1a1d-40ce-84d0-25860624a912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16625 shortened and 506786 unshortened URLs\n"
     ]
    }
   ],
   "source": [
    "shortened_urls = URLs[URLs[\"domain\"].isin(url_shorteners)]\n",
    "unshortened_urls = URLs[~URLs[\"domain\"].isin(url_shorteners)]\n",
    "print(f\"{len(shortened_urls)} shortened and {len(unshortened_urls)} unshortened URLs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c206e0f-0386-4abe-8afa-535cec702266",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"midterm_candidates_URLs.csv.gzip\"\n",
    "shortened_urls[\"url\"].to_csv(Path(dst, \"tmp\", fname),\n",
    "                    compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821d1ed5-d8cb-4b92-95f8-85b8de8e6672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the URLs that need unravelling to the remote server\n",
    "! rsync -avze ssh ../data/tmp/midterm_candidates_URLs.csv.gzip jlasser@medea:/home/jlasser/Honesty-project/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa61d9db-1c2a-4292-aa6b-b4e65f093b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run on the server: follow all the URLs. Note: this takes about a day per 100k URLs\n",
    "#cd Honesty-project/code/\n",
    "! mkdir -p ../data/midterm_candidates_unraveled_urls\n",
    "! python ../../../../utilities/unravel_urls/unravel_urls.py midterm_candidates_URLs.csv.gzip -dst ../data/midterm_candidates_unraveled_urls/ -v 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa2980a-c04f-4eda-bbfa-5fc1d62e0de2",
   "metadata": {},
   "source": [
    "## Load unraveled URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c9211c-27b1-4ee0-b50b-406ca2e7b297",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load unraveled URLs from server (they are stored in batches of 1000 URLs\n",
    "! rsync -avze ssh jlasser@medea:/home/jlasser/Honesty-project/data/midterm_candidates_unraveled_urls/ ../data/tmp/midterm_candidates_unraveled_urls/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "176de985-6f09-43d0-8047-d06e95f9497b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/167\n"
     ]
    }
   ],
   "source": [
    "files = listdir(Path(src, \"tmp\", \"midterm_candidates_unraveled_urls\"))\n",
    "unraveled_urls = pd.DataFrame()\n",
    "for i,f in enumerate(files):\n",
    "    if i%1000 == 0:\n",
    "        print(f\"{i}/{len(files)}\")\n",
    "    tmp = pd.read_csv(Path(src, f), compression=\"gzip\")\n",
    "    unraveled_urls = pd.concat([unraveled_urls, tmp])\n",
    "unraveled_urls = unraveled_urls.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1575e4-abd3-4e33-89b3-29a87f250e72",
   "metadata": {},
   "source": [
    "## Add hosts from timeouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca50a23a-0d2b-4269-ab1e-fd41ec8a6592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "689 timeouts (4.32%)\n"
     ]
    }
   ],
   "source": [
    "timeouts = len(unraveled_urls) - len(unraveled_urls[\"status_code\"].dropna())\n",
    "print(\"{} timeouts ({:1.2f}%)\".format(\\\n",
    "        timeouts,\n",
    "        (timeouts / len(unraveled_urls[\"status_code\"].dropna()) * 100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ad48ac7-8990-4d19-a1f4-89dc808b2378",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_host(unraveled_url):\n",
    "    if unraveled_url == unraveled_url and unraveled_url.startswith(\"Cannot\"):\n",
    "        host = unraveled_url.split(\" \")[4].split(\":\")[0]\n",
    "        return host\n",
    "    else:\n",
    "        return unraveled_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2ab995e-1caa-42bb-9dd8-be358926de80",
   "metadata": {},
   "outputs": [],
   "source": [
    "unraveled_urls[\"unraveled_url\"] = unraveled_urls[\"unraveled_url\"].apply(extract_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e64a579c-f80c-4d8b-90b4-8ce5d244d2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"midterm_candidates_unraveled_urls.csv.xz\"\n",
    "\n",
    "unraveled_urls.to_csv(\n",
    "    Path(dst, \"tmp\", fname),\n",
    "    index=False,\n",
    "    compression=\"xz\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
