{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8adf6aed-ce20-45b1-afd3-8bd4e8d4e270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d081deb-8b5b-4a92-86fb-8f36fa2a4738",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../data\"\n",
    "dst = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8c1c77-c6bc-4b76-b552-e44d6600a756",
   "metadata": {},
   "source": [
    "# Process timelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b6ef8-bd28-414e-bb63-9fdc8be17303",
   "metadata": {},
   "source": [
    "## Transform tweet table into URL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0a2b772-6192-49dc-a94e-0349c28e09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"combined_midterm_candidate_timelines_2022-01-01_to_2023-05-01_clean.csv.gzip\"\n",
    "cols = [\"id\", \"author_id\", \"created_at\", \"expanded_urls\", \"retweeted\", \"quoted\",\n",
    "        \"reply\", \"text\", \"retweet_count\", \"reply_count\", \"like_count\",\n",
    "        \"quote_count\"]\n",
    "tweets = pd.read_csv(\n",
    "    Path(src, \"raw\", fname),\n",
    "    dtype={\"id\":str, \"author_id\":str},\n",
    "    parse_dates=[\"created_at\"],\n",
    "    compression=\"gzip\",\n",
    "    usecols=cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c4d4424-ac49-4e94-bc50-7e9e5fa41593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the URL lists\n",
    "tweets[\"expanded_urls\"] = tweets[\"expanded_urls\"].fillna(\"[]\")\n",
    "tweets[\"expanded_urls\"] = tweets[\"expanded_urls\"].apply(lambda x: eval(x))\n",
    "tweets[\"has_url\"] = tweets[\"expanded_urls\"].apply(lambda x: len(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38e0dc6b-7898-478d-9fb7-fde508f7ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"N_urls\"] = tweets[\"expanded_urls\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57429fad-0f36-4cab-adb7-cc6c9e0401eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand only entries with multiple URLs\n",
    "multiple_urls = tweets[tweets[\"N_urls\"] > 1]\n",
    "expanded_urls = pd.DataFrame()\n",
    "for idx, entry in multiple_urls.iterrows():\n",
    "    row = {key:val for key, val in entry.items()}\n",
    "    expanded_urls = pd.concat([expanded_urls, pd.DataFrame(row)])\n",
    "    \n",
    "expanded_urls = expanded_urls.set_index(\"id\")\n",
    "urls = tweets.copy()\n",
    "urls = urls.set_index(\"id\")\n",
    "# drop entries with mutiple URLs\n",
    "urls = urls.drop(multiple_urls[\"id\"].values)\n",
    "# add expanded entries with one line for each URL\n",
    "urls = pd.concat([urls, expanded_urls])\n",
    "urls = urls.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c60fe03-9571-492b-a1f8-61008691e379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1084776"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce081166-fb1a-498e-86fe-b3fc26a7a128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1212940"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e328a371-ee09-4bc5-a1fa-78b7a249eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, some URLs are stored as singular entries of a list, and some as string.\n",
    "# empty entries are stored as empty list. Below we streamline URL entries such\n",
    "# that every entry is a single string\n",
    "def extract_URL_from_list(entry):\n",
    "    if len(entry) == 0:\n",
    "        return np.nan\n",
    "    elif len(entry) == 1:\n",
    "        return entry[0]\n",
    "    else:\n",
    "        return entry\n",
    "    \n",
    "urls[\"expanded_urls\"] = urls[\"expanded_urls\"].apply(extract_URL_from_list)\n",
    "urls = urls.rename(columns={\"expanded_urls\":\"url\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c30fa1d8-80c4-4d0e-9191-72439864b624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 85207 duplicate URL entries\n"
     ]
    }
   ],
   "source": [
    "# some tweets contain the same URL twice. We drop these\n",
    "N = len(urls)\n",
    "urls = urls.drop_duplicates(subset=[\"id\", \"url\"])\n",
    "print(f\"dropped {N - len(urls)} duplicate URL entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "192693aa-b9dd-4bee-8791-1c5c1ba83e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8808071a-840e-4c3a-b3b7-94bfc8a1bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the outcome\n",
    "fname = \"combined_midterm_candidate_timelines_2022-01-01_to_2023-05-01_clean_urls.csv.gzip\"\n",
    "urls.to_csv(Path(dst, \"tmp\", fname), compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0c96ab0-7ac1-4345-9302-4b48e771bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data frame with the expanded URLs\n",
    "fname = \"combined_midterm_candidate_timelines_2022-01-01_to_2023-05-01_clean_urls.csv.gzip\"\n",
    "cols = [\"id\", \"author_id\", \"created_at\", \"url\", \"retweeted\",\n",
    "        \"quoted\", \"reply\", \"has_url\"]\n",
    "urls = pd.read_csv(\n",
    "    Path(src, \"tmp\", fname),\n",
    "    compression=\"gzip\",\n",
    "    usecols=cols,\n",
    "    parse_dates=[\"created_at\"],\n",
    "    dtype={\"author_id\":str, \"id\":str}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e6decc5-5737-4f07-84a9-e3509f4c39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add handle back to data frame\n",
    "fname = \"candidate_twitter_profiles.csv\"\n",
    "cols = [\"author_id\", \"handle\"]\n",
    "users = pd.read_csv(\n",
    "    Path(src, \"tmp\", fname),\n",
    "    dtype={\"author_id\":str},\n",
    "    usecols=cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a539e4a3-c730-4e6d-99f7-6502e64ada1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.merge(\n",
    "    urls,\n",
    "    users,\n",
    "    how=\"left\",\n",
    "    left_on=\"author_id\",\n",
    "    right_on=\"author_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cce10122-38b4-415d-bdd2-ca72e5e1aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "del users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d37ba8-8ff9-4f16-861c-1cf3b94546aa",
   "metadata": {},
   "source": [
    "## Add engagement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66546f2b-59b5-4924-a61f-2b44cce09b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the public metrics information for the collected tweets\n",
    "fname = \"combined_midterm_candidate_timelines_2022-01-01_to_2023-05-01_clean.csv.gzip\"\n",
    "tweet_metrics = pd.read_csv(Path(src, \"raw\", fname),\n",
    "                 compression=\"gzip\",\n",
    "                 usecols=[\"id\", \"retweet_count\",\n",
    "                          \"reply_count\", \"like_count\", \"quote_count\"],\n",
    "                dtype={\"id\":str})\n",
    "tweet_metrics = tweet_metrics.drop_duplicates(subset=\"id\")\n",
    "# merge the tweet metrics with the tweet data frame\n",
    "urls = pd.merge(urls, tweet_metrics, how=\"left\", left_on=\"id\", right_on=\"id\")\n",
    "del tweet_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ddc9f-a1fe-40a7-b013-5a0f7a2f93ca",
   "metadata": {},
   "source": [
    "## Add unraveled URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bff59ea5-2f42-4bad-b0e1-d105cae61dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the list of originally shortened URLs with their expansions to their true\n",
    "# destination\n",
    "fname = \"midterm_candidates_unraveled_urls.csv.xz\"\n",
    "unraveled_urls = pd.read_csv(\n",
    "    Path(src, \"tmp\", fname), \n",
    "    compression=\"xz\",\n",
    "    usecols=[\"url\", \"unraveled_url\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4760d511-dafb-4c43-b7fe-2eb731b05475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add URL information\n",
    "urls = pd.merge(\n",
    "    urls,\n",
    "    unraveled_urls[[\"url\", \"unraveled_url\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"url\",\n",
    "    right_on=\"url\"\n",
    ")\n",
    "\n",
    "# add indicator of whether the URL was originally shortened\n",
    "urls[\"shortened_url\"] = False\n",
    "urls.loc[urls[\"unraveled_url\"].dropna().index, \"shortened_url\"] = True\n",
    "\n",
    "# replace the shortened URL with the unraveled URL\n",
    "urls.loc[urls[\"unraveled_url\"].dropna().index, \"url\"] = \\\n",
    "    urls.loc[urls[\"unraveled_url\"].dropna().index, \"unraveled_url\"]\n",
    "urls = urls.drop(columns=[\"unraveled_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0e09c45-220f-47b7-925c-80b4b58b3868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract the domain from the URL. Note: a few \"found malformed URL\" warnings\n",
    "# are acceptable\n",
    "def extract_domain(url):\n",
    "    '''Given an ULR, extracts the domain name in the form XXXXX.YY'''\n",
    "    if url != url:\n",
    "        return np.nan\n",
    "    # reformat entries that have the domain after a general name in parantheses\n",
    "    if url.find('(') > 0:\n",
    "        url = url.split('(')[-1]\n",
    "        url = url.strip(')')\n",
    "    # trailing \"/\" and spaces\n",
    "    url = url.strip('/').strip()\n",
    "    # transform all domains to lowercase\n",
    "    url = url.lower()\n",
    "    # remove any white spaces\n",
    "    url = url.replace(' ', '')\n",
    "    # if present: remove the protocol\n",
    "    if url.startswith((\"http\", \"https\")):\n",
    "        try:\n",
    "            url = url.split('//')[1]\n",
    "        except IndexError:\n",
    "            print(f\"found malformed URL {url}\")\n",
    "            return np.nan\n",
    "    # remove \"www.\" \n",
    "    url = url.replace('www.', '')\n",
    "    url = url.split(\"/\")[0]\n",
    "    return url\n",
    "\n",
    "urls[\"domain\"] = urls[\"url\"].apply(extract_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d92c3d-af22-4aeb-ad9e-5c0d6f509d7b",
   "metadata": {},
   "source": [
    "## Add NewsGuard nutrition scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39628cb-0dbe-4bd2-bf73-3d13dfe98565",
   "metadata": {},
   "source": [
    "Newsguard rating threshold to label a domain as \"untrustworthy\": 60 (see [description](https://www.newsguardtech.com/ratings/rating-process-criteria/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecabd883-b703-4cdf-9000-a54211eb2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfc15a24-723e-41c7-ab33-3911ede3d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the nutrition labels from the day of the midterm election: 2022-11-08\n",
    "fname = \"metadata-2022110801.csv\"\n",
    "cols = [\"Domain\", \"Score\", \"Last Updated\"]\n",
    "NG_scores = pd.read_csv(Path(src, \"raw\", fname), usecols=cols)\n",
    "# if more than one score exists for the same domain, keep the most recent one\n",
    "NG_scores = NG_scores.sort_values(by=[\"Domain\",\"Last Updated\"], ascending=False)\n",
    "NG_scores = NG_scores.drop_duplicates(subset=[\"Domain\"])\n",
    "NG_scores = NG_scores.rename(columns={\"Domain\":\"domain\", \"Score\":\"NG_score\"})\n",
    "NG_scores = NG_scores.drop(columns=[\"Last Updated\"])\n",
    "\n",
    "# threshold scores to define untrustworthy domains\n",
    "NG_scores[\"NG_untrustworthy\"] = 0\n",
    "NG_scores.loc[NG_scores[NG_scores[\"NG_score\"] < threshold].index, \"NG_untrustworthy\"] = 1\n",
    "\n",
    "# add the nutrition information to the tweet data table\n",
    "urls = pd.merge(urls, NG_scores,\n",
    "         left_on=\"domain\", right_on=\"domain\", how=\"left\")\n",
    "del NG_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b68e7d8-1e3b-4ca1-84a7-39859f4d8dd2",
   "metadata": {},
   "source": [
    "## Data export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "210ca8a7-8c20-4342-ae5b-d4edd66926f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL data frame\n",
    "fname = \"midterm_URLs_2022-01-01_to_2023-05-01.csv.gzip\"\n",
    "urls = urls[urls[\"has_url\"] == True]\n",
    "urls = urls.drop(columns=[\"url\", \"has_url\"])\n",
    "urls.to_csv(Path(dst, \"tmp\", fname), index=False, compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
