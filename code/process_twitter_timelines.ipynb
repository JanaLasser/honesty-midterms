{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8adf6aed-ce20-45b1-afd3-8bd4e8d4e270",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import linregress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d081deb-8b5b-4a92-86fb-8f36fa2a4738",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = \"../data\"\n",
    "dst = \"../data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8c1c77-c6bc-4b76-b552-e44d6600a756",
   "metadata": {},
   "source": [
    "# Process timelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16b6ef8-bd28-414e-bb63-9fdc8be17303",
   "metadata": {},
   "source": [
    "## Transform tweet table into URL table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0a2b772-6192-49dc-a94e-0349c28e09f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"combined_midterm_candidate_timelines_2022-01-01_to_2023-05-01_clean.csv.gzip\"\n",
    "cols = [\"id\", \"author_id\", \"created_at\", \"expanded_urls\", \"retweeted\", \"quoted\",\n",
    "        \"reply\", \"text\", \"retweet_count\", \"reply_count\", \"like_count\",\n",
    "        \"quote_count\"]\n",
    "tweets = pd.read_csv(\n",
    "    Path(src, fname),\n",
    "    dtype={\"id\":str, \"author_id\":str},\n",
    "    parse_dates=[\"created_at\"],\n",
    "    compression=\"gzip\",\n",
    "    usecols=cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c4d4424-ac49-4e94-bc50-7e9e5fa41593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the URL lists\n",
    "tweets[\"expanded_urls\"] = tweets[\"expanded_urls\"].fillna(\"[]\")\n",
    "tweets[\"expanded_urls\"] = tweets[\"expanded_urls\"].apply(lambda x: eval(x))\n",
    "tweets[\"has_url\"] = tweets[\"expanded_urls\"].apply(lambda x: len(x) > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38e0dc6b-7898-478d-9fb7-fde508f7ce2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[\"N_urls\"] = tweets[\"expanded_urls\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57429fad-0f36-4cab-adb7-cc6c9e0401eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand only entries with multiple URLs\n",
    "multiple_urls = tweets[tweets[\"N_urls\"] > 1]\n",
    "expanded_urls = pd.DataFrame()\n",
    "for idx, entry in multiple_urls.iterrows():\n",
    "    row = {key:val for key, val in entry.items()}\n",
    "    expanded_urls = pd.concat([expanded_urls, pd.DataFrame(row)])\n",
    "    \n",
    "expanded_urls = expanded_urls.set_index(\"id\")\n",
    "urls = tweets.copy()\n",
    "urls = urls.set_index(\"id\")\n",
    "# drop entries with mutiple URLs\n",
    "urls = urls.drop(multiple_urls[\"id\"].values)\n",
    "# add expanded entries with one line for each URL\n",
    "urls = pd.concat([urls, expanded_urls])\n",
    "urls = urls.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c60fe03-9571-492b-a1f8-61008691e379",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1084776"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce081166-fb1a-498e-86fe-b3fc26a7a128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1212940"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e328a371-ee09-4bc5-a1fa-78b7a249eed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, some URLs are stored as singular entries of a list, and some as string.\n",
    "# empty entries are stored as empty list. Below we streamline URL entries such\n",
    "# that every entry is a single string\n",
    "def extract_URL_from_list(entry):\n",
    "    if len(entry) == 0:\n",
    "        return np.nan\n",
    "    elif len(entry) == 1:\n",
    "        return entry[0]\n",
    "    else:\n",
    "        return entry\n",
    "    \n",
    "urls[\"expanded_urls\"] = urls[\"expanded_urls\"].apply(extract_URL_from_list)\n",
    "urls = urls.rename(columns={\"expanded_urls\":\"url\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c30fa1d8-80c4-4d0e-9191-72439864b624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropped 85207 duplicate URL entries\n"
     ]
    }
   ],
   "source": [
    "# some tweets contain the same URL twice. We drop these\n",
    "N = len(urls)\n",
    "urls = urls.drop_duplicates(subset=[\"id\", \"url\"])\n",
    "print(f\"dropped {N - len(urls)} duplicate URL entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "192693aa-b9dd-4bee-8791-1c5c1ba83e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "del tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8808071a-840e-4c3a-b3b7-94bfc8a1bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the outcome\n",
    "fname = \"combined_midterm_candidate_timelines_2022-01-01_to_2023-05-01_clean_urls.csv.gzip\"\n",
    "urls.to_csv(Path(dst, fname), compression=\"gzip\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a0c96ab0-7ac1-4345-9302-4b48e771bc22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data frame with the expanded URLs\n",
    "fname = \"combined_midterm_candidate_timelines_2022-01-01_to_2023-05-01_clean_urls.csv.gzip\"\n",
    "cols = [\"id\", \"author_id\", \"created_at\", \"url\", \"retweeted\",\n",
    "        \"quoted\", \"reply\", \"has_url\"]\n",
    "urls = pd.read_csv(\n",
    "    Path(src, fname),\n",
    "    compression=\"gzip\",\n",
    "    usecols=cols,\n",
    "    parse_dates=[\"created_at\"],\n",
    "    dtype={\"author_id\":str, \"id\":str}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e6decc5-5737-4f07-84a9-e3509f4c39b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add handle back to data frame\n",
    "fname = \"candidate_twitter_profiles.csv\"\n",
    "cols = [\"author_id\", \"handle\"]\n",
    "users = pd.read_csv(\n",
    "    Path(src, fname),\n",
    "    dtype={\"author_id\":str},\n",
    "    usecols=cols\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a539e4a3-c730-4e6d-99f7-6502e64ada1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = pd.merge(\n",
    "    urls,\n",
    "    users,\n",
    "    how=\"left\",\n",
    "    left_on=\"author_id\",\n",
    "    right_on=\"author_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cce10122-38b4-415d-bdd2-ca72e5e1aa93",
   "metadata": {},
   "outputs": [],
   "source": [
    "del users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d37ba8-8ff9-4f16-861c-1cf3b94546aa",
   "metadata": {},
   "source": [
    "## Add engagement metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66546f2b-59b5-4924-a61f-2b44cce09b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the public metrics information for the collected tweets\n",
    "fname = \"combined_midterm_candidate_timelines_2022-01-01_to_2023-05-01_clean.csv.gzip\"\n",
    "tweet_metrics = pd.read_csv(Path(src, fname),\n",
    "                 compression=\"gzip\",\n",
    "                 usecols=[\"id\", \"retweet_count\",\n",
    "                          \"reply_count\", \"like_count\", \"quote_count\"],\n",
    "                dtype={\"id\":str})\n",
    "tweet_metrics = tweet_metrics.drop_duplicates(subset=\"id\")\n",
    "# merge the tweet metrics with the tweet data frame\n",
    "urls = pd.merge(urls, tweet_metrics, how=\"left\", left_on=\"id\", right_on=\"id\")\n",
    "del tweet_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477ddc9f-a1fe-40a7-b013-5a0f7a2f93ca",
   "metadata": {},
   "source": [
    "## Add unraveled URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bff59ea5-2f42-4bad-b0e1-d105cae61dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the list of originally shortened URLs with their expansions to their true\n",
    "# destination\n",
    "fname = \"midterm_candidates_unraveled_urls.csv.xz\"\n",
    "unraveled_urls = pd.read_csv(\n",
    "    Path(src, fname), \n",
    "    compression=\"xz\",\n",
    "    usecols=[\"url\", \"unraveled_url\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4760d511-dafb-4c43-b7fe-2eb731b05475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add URL information\n",
    "urls = pd.merge(\n",
    "    urls,\n",
    "    unraveled_urls[[\"url\", \"unraveled_url\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"url\",\n",
    "    right_on=\"url\"\n",
    ")\n",
    "\n",
    "# add indicator of whether the URL was originally shortened\n",
    "urls[\"shortened_url\"] = False\n",
    "urls.loc[urls[\"unraveled_url\"].dropna().index, \"shortened_url\"] = True\n",
    "\n",
    "# replace the shortened URL with the unraveled URL\n",
    "urls.loc[urls[\"unraveled_url\"].dropna().index, \"url\"] = \\\n",
    "    urls.loc[urls[\"unraveled_url\"].dropna().index, \"unraveled_url\"]\n",
    "urls = urls.drop(columns=[\"unraveled_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e0e09c45-220f-47b7-925c-80b4b58b3868",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# extract the domain from the URL. Note: a few \"found malformed URL\" warnings\n",
    "# are acceptable\n",
    "def extract_domain(url):\n",
    "    '''Given an ULR, extracts the domain name in the form XXXXX.YY'''\n",
    "    if url != url:\n",
    "        return np.nan\n",
    "    # reformat entries that have the domain after a general name in parantheses\n",
    "    if url.find('(') > 0:\n",
    "        url = url.split('(')[-1]\n",
    "        url = url.strip(')')\n",
    "    # trailing \"/\" and spaces\n",
    "    url = url.strip('/').strip()\n",
    "    # transform all domains to lowercase\n",
    "    url = url.lower()\n",
    "    # remove any white spaces\n",
    "    url = url.replace(' ', '')\n",
    "    # if present: remove the protocol\n",
    "    if url.startswith((\"http\", \"https\")):\n",
    "        try:\n",
    "            url = url.split('//')[1]\n",
    "        except IndexError:\n",
    "            print(f\"found malformed URL {url}\")\n",
    "            return np.nan\n",
    "    # remove \"www.\" \n",
    "    url = url.replace('www.', '')\n",
    "    url = url.split(\"/\")[0]\n",
    "    return url\n",
    "\n",
    "urls[\"domain\"] = urls[\"url\"].apply(extract_domain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d92c3d-af22-4aeb-ad9e-5c0d6f509d7b",
   "metadata": {},
   "source": [
    "## Add NewsGuard nutrition scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39628cb-0dbe-4bd2-bf73-3d13dfe98565",
   "metadata": {},
   "source": [
    "Newsguard rating threshold to label a domain as \"untrustworthy\": 60 (see [description](https://www.newsguardtech.com/ratings/rating-process-criteria/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ecabd883-b703-4cdf-9000-a54211eb2946",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bfc15a24-723e-41c7-ab33-3911ede3d4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the nutrition labels from the day of the midterm election: 2022-11-08\n",
    "fname = \"metadata-2022110801.csv\"\n",
    "cols = [\"Domain\", \"Score\", \"Last Updated\"]\n",
    "NG_scores = pd.read_csv(Path(src, fname), usecols=cols)\n",
    "# if more than one score exists for the same domain, keep the most recent one\n",
    "NG_scores = NG_scores.sort_values(by=[\"Domain\",\"Last Updated\"], ascending=False)\n",
    "NG_scores = NG_scores.drop_duplicates(subset=[\"Domain\"])\n",
    "NG_scores = NG_scores.rename(columns={\"Domain\":\"domain\", \"Score\":\"NG_score\"})\n",
    "NG_scores = NG_scores.drop(columns=[\"Last Updated\"])\n",
    "\n",
    "# threshold scores to define untrustworthy domains\n",
    "NG_scores[\"NG_untrustworthy\"] = 0\n",
    "NG_scores.loc[NG_scores[NG_scores[\"NG_score\"] < threshold].index, \"NG_untrustworthy\"] = 1\n",
    "\n",
    "# add the nutrition information to the tweet data table\n",
    "urls = pd.merge(urls, NG_scores,\n",
    "         left_on=\"domain\", right_on=\"domain\", how=\"left\")\n",
    "del NG_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bd1e0c-907b-4deb-92de-2e7ae835b758",
   "metadata": {},
   "source": [
    "## Tweet length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a5904da4-721a-4b11-9ecf-4c0a7d95a4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the cleaned timeline-data\n",
    "fname = \"combined_midterm_candidate_timelines_2022-01-01_to_2023-05-01_clean.csv.gzip\"\n",
    "cols = [\"id\", \"text\"]\n",
    "texts = pd.read_csv(\n",
    "    Path(src, fname),\n",
    "    compression=\"gzip\",\n",
    "    usecols=cols,\n",
    "    dtype={\"id\":str, \"text\":str}\n",
    ")\n",
    "\n",
    "texts[\"tweet_length\"] = texts[\"text\"].apply(lambda x: len(x))\n",
    "\n",
    "urls = pd.merge(\n",
    "    urls,\n",
    "    texts[[\"id\", \"tweet_length\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"id\",\n",
    "    right_on=\"id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13e74f7-24f2-4851-905a-6edcf80efd63",
   "metadata": {},
   "source": [
    "## Add belief-speaking and fact-speaking scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b5a56637-8f9e-49c9-a9dd-a4bc08466f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the embedding scores for belief-speaking and truth-seeking\n",
    "fname = \"combined_midterm_candidate_timelines_2022-01-01_to_2023-05-01_honesty_component_scores.csv.gzip\"\n",
    "honesty_scores = pd.read_csv(\n",
    "    Path(src, fname),\n",
    "    dtype={\"id\":str}, \n",
    "    compression=\"gzip\"\n",
    ").rename(columns={\"avg_fact_score\":\"avg_fact_score_raw\", \"avg_belief_score\":\"avg_belief_score_raw\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "92ac0440-9ec3-4c0b-9fdd-294b5f261cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "honesty_scores = pd.merge(\n",
    "    honesty_scores,\n",
    "    urls[[\"id\", \"tweet_length\"]],\n",
    "    how=\"left\",\n",
    "    left_on=\"id\",\n",
    "    right_on=\"id\"\n",
    ")\n",
    "honesty_scores = honesty_scores.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3b95f230-4b58-4644-97cd-2081de4e97c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "belief-speaking slope: 0.00021486802643395315, intercept: 0.6849102858687011\n",
      "fact-seeking slope: 0.00032929502778065143, intercept: 0.5772773113543177\n"
     ]
    }
   ],
   "source": [
    "# correct the similarity scores for tweet-length effects\n",
    "slope_belief, intercept_belief, rval_belief, pval_belief, stderr_belief = \\\n",
    "    linregress(honesty_scores[\"tweet_length\"], honesty_scores[\"avg_belief_score_raw\"])\n",
    "print(f\"belief-speaking slope: {slope_belief}, intercept: {intercept_belief}\")\n",
    "\n",
    "def predict_belief_similarity(tweet_length):\n",
    "    return intercept_belief + slope_belief * tweet_length\n",
    "\n",
    "slope_fact, intercept_fact, rval_fact, pval_fact, stderr_fact = \\\n",
    "    linregress(honesty_scores[\"tweet_length\"], honesty_scores[\"avg_fact_score_raw\"])\n",
    "print(f\"fact-seeking slope: {slope_fact}, intercept: {intercept_fact}\")\n",
    "\n",
    "def predict_fact_similarity(tweet_length):\n",
    "    return intercept_fact + slope_fact * tweet_length\n",
    "\n",
    "honesty_scores[\"avg_belief_score\"] = honesty_scores\\\n",
    "    .apply(lambda x: x[\"avg_belief_score_raw\"] - predict_belief_similarity(x[\"tweet_length\"]), axis=1)\n",
    "\n",
    "honesty_scores[\"avg_fact_score\"] = honesty_scores\\\n",
    "    .apply(lambda x: x[\"avg_fact_score_raw\"] - predict_fact_similarity(x[\"tweet_length\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d5483013-086f-46c8-bf3c-728f79a15666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the raw and corrected scores with the url data frame\n",
    "cols = [\"id\", \"avg_fact_score_raw\", \"avg_fact_score\", \"avg_belief_score_raw\", \"avg_belief_score\"]\n",
    "urls = pd.merge(\n",
    "    honesty_scores[cols], \n",
    "    urls, \n",
    "    how=\"right\", \n",
    "    left_on=\"id\", \n",
    "    right_on=\"id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb5e8d-f880-40f0-9613-298bba0e54b0",
   "metadata": {},
   "source": [
    "## Create a tweet data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "53e50bda-2145-46ee-8759-862170a9b980",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_cols = [\"id\", \"author_id\", \"handle\", \"created_at\", \"retweeted\", \"quoted\", \"reply\",\n",
    "              \"has_url\", \"tweet_length\", \"avg_belief_score\", \"avg_fact_score\"]\n",
    "tweets = urls[tweet_cols].drop_duplicates(subset=[\"id\"]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e34546c7-f559-4e5f-bdbf-d0948f785a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average NewsGuard score\n",
    "average_scores = urls[[\"id\", \"NG_score\"]]\\\n",
    "    .groupby(\"id\")\\\n",
    "    .agg(\"mean\")\n",
    "\n",
    "average_scores[\"NG_unreliable\"] = np.nan\n",
    "average_scores.loc[average_scores[\\\n",
    "            average_scores[\"NG_score\"] < 60].index, \"NG_unreliable\"] = 1\n",
    "average_scores.loc[average_scores[\\\n",
    "            average_scores[\"NG_score\"] >= 60].index, \"NG_unreliable\"] = 0\n",
    "\n",
    "tweets = pd.merge(tweets, average_scores, how=\"left\", left_on=\"id\", right_on=\"id\")\n",
    "del average_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c3b7f8-49a0-470d-9198-65b1281da41f",
   "metadata": {},
   "source": [
    "## Create a user data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acb56f58-5629-4878-8fe9-435edb0e9748",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = tweets[[\"author_id\", \"handle\", \"id\"]]\\\n",
    "    .groupby([\"author_id\", \"handle\"])\\\n",
    "    .agg(\"count\")\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={\"id\":\"N_tweets\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3cd0a-9154-4402-9409-48e8f83631a4",
   "metadata": {},
   "source": [
    "### Add account stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cc365b8b-c448-4ee7-a586-59a92a797446",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"candidate_twitter_profiles.csv\"\n",
    "cols = [\"followers_count\", \"following_count\", \"tweet_count\", \"created_at\", \n",
    "        \"author_id\"]\n",
    "account_stats = pd.read_csv(\n",
    "    Path(src, fname),\n",
    "    parse_dates=[\"created_at\"],\n",
    "    usecols=cols,\n",
    "    dtype={\"author_id\":str}\n",
    ")\n",
    "\n",
    "users = pd.merge(users, account_stats, how=\"left\", left_on=\"author_id\", right_on=\"author_id\")\n",
    "del account_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe84e66-463d-42be-8876-e523499de02e",
   "metadata": {},
   "source": [
    "### Add share of untrustworthy domains (NewsGuard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "796e32d6-08fb-4d68-a4f6-64001094da47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author_id</th>\n",
       "      <th>NG_unreliable_sum</th>\n",
       "      <th>NG_unreliable_count</th>\n",
       "      <th>NG_unreliable_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100049648</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002275817227980800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             author_id  NG_unreliable_sum  NG_unreliable_count  \\\n",
       "0            100049648                1.0                    4   \n",
       "1  1002275817227980800                0.0                   13   \n",
       "\n",
       "   NG_unreliable_share  \n",
       "0                 0.25  \n",
       "1                 0.00  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = [\"author_id\", \"NG_unreliable\"]\n",
    "unreliable_user_count = tweets[tweets[\"retweeted\"] == False][cols]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .agg([\"sum\", \"count\"])\n",
    "\n",
    "unreliable_user_count[\"NG_unreliable_share\"] = \\\n",
    "    unreliable_user_count[\"NG_unreliable\"][\"sum\"] / \\\n",
    "    unreliable_user_count[\"NG_unreliable\"][\"count\"]\n",
    "    \n",
    "# flatten the hierarchical indices\n",
    "unreliable_user_count = unreliable_user_count.reset_index()\n",
    "unreliable_user_count.columns = ['_'.join(col).strip(\"_\") \\\n",
    "                            for col in unreliable_user_count.columns.values]\n",
    "\n",
    "unreliable_user_count.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f6e7935f-67fc-47bf-a9e9-95d865e82de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"NG_unreliable_share\", \"author_id\"]\n",
    "users = pd.merge(\n",
    "    users, \n",
    "    unreliable_user_count[cols],\n",
    "    how=\"left\",\n",
    "    left_on=\"author_id\",\n",
    "    right_on=\"author_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b7bd51-ab2d-45a1-aba1-62d8bc63c906",
   "metadata": {},
   "source": [
    "### Add average NewsGuard score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "286dd831-9e7f-4388-8e97-ebca18ac096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_NG_scores = tweets[tweets[\"retweeted\"] == False][[\"author_id\", \"NG_score\"]]\\\n",
    "    .groupby(\"author_id\")\\\n",
    "    .mean()\\\n",
    "    .reset_index()\\\n",
    "    .rename(columns={\"NG_score\":\"NG_score_mean\"})\n",
    "users = pd.merge(\n",
    "    users, \n",
    "    average_NG_scores, \n",
    "    how=\"left\", \n",
    "    left_on=\"author_id\", \n",
    "    right_on=\"author_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b68e7d8-1e3b-4ca1-84a7-39859f4d8dd2",
   "metadata": {},
   "source": [
    "## Data exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "210ca8a7-8c20-4342-ae5b-d4edd66926f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL data frame\n",
    "fname = \"midterm_URLs_2022-01-01_to_2023-05-01.csv.gzip\"\n",
    "urls = urls[urls[\"has_url\"] == True]\n",
    "urls = urls.drop(columns=[\"url\", \"has_url\"])\n",
    "urls.to_csv(Path(dst, fname), index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d9fe88c1-e0a3-435c-80e0-542db553c020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user data frame\n",
    "fname = \"midterm_users_2022-01-01_to_2023-05-01.csv\"\n",
    "users.to_csv(Path(dst, fname), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7405ee03-a0ad-4995-b864-6fa8e9c5717f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweet data frame\n",
    "fname = \"midterm_tweets_2022-01-01_to_2023-05-01.csv.gzip\"\n",
    "tweets.to_csv(Path(dst, fname), index=False, compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
